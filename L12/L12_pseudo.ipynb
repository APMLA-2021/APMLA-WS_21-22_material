{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models with LDA\n",
    "\n",
    "In this exercise, we will learn how to apply and visualize topic models in Python. \n",
    "We will use the package `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import _stop_words as stop_words\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: processing bag of words representation and analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a toy example to illustrate how to preprocess and visualize data. Consider a set of four documents, each consisting of one single sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"I like to eat broccoli and bananas; Broccoli and bananas are healthy.\"\n",
    "doc2 = \"I eat broccoli smoothie and bananas for breakfast.\"\n",
    "doc3 = \"Hamsters and kittens are cute.\"\n",
    "doc4 = \"My sister says she wants to adopt two cute kittens, but we already have three hamsters at home.\"\n",
    "\n",
    "# complete list of documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Tokenize the document\n",
    "These are the steps that this is doing:\n",
    "1. Remove punctuation.\n",
    "2. Remove \"stop words\".\n",
    "3. Remove low-frequency words.\n",
    "4. Create the dictionary.\n",
    "5. Create the bag-of-words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_stop = list(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_documents(documents,stoplist,max_df0=0.80, min_df0=0.02,print_vocabulary=False,outfolder=None,output_vocabulary_fname='vocabulary.dat'):\n",
    "    '''\n",
    "    From a list of documents raw text build a matrix DxV\n",
    "    D: number of docs\n",
    "    V: size of the vocabulary, i.e. number of unique terms found in the whole set of docs\n",
    "    '''\n",
    "    count_vect = CountVectorizer(stop_words=stoplist,max_df=max_df0, min_df=min_df0)\n",
    "    corpus = # FILL\n",
    "\n",
    "    vocabulary_dict=# FILL\n",
    "    vocabulary_list=[(key,value) for # FILL]\n",
    "    vocabulary_list.sort(# FILL)\n",
    "        \n",
    "    if print_vocabulary==True:output_vocabulary(outfolder,count_vect,outfile=output_vocabulary_fname)\n",
    "    return corpus,vocabulary_list,vocabulary_dict,count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus,vocabulary_list,vocabulary_dict,count_vect=# FILL\n",
    "print(corpus.shape, len(vocabulary_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D,V=corpus.shape\n",
    "D,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Run LDA\n",
    "\n",
    "We now apply Latent Dirichlet Allocation (LDA) to our preprocessed corpus. The idea behind LDA is that each document can be understood as a mixture of \"topics\". For instance, documents 1 and 2 are about food because they contain the words \"broccoli\", \"bananas\", and \"eat\"; documents 3 and 4 are about animals (\"kittens\", \"hamsters\", \"cute\"); and document 5 is about both animals (\"hamsters\") and food (\"broccoli\"). LDA unveils these topics automatically from the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LDA\n",
    "n_topics = # FILL\n",
    "lda_model = # FILL\n",
    "topic_proportions = # FILL\n",
    "topics = # FILL\n",
    "\n",
    "# Print log-likelihood\n",
    "print('\\nLog likelihood: ' + str(lda_model.score(# FILL)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Analyze the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the resulting matrices\n",
    "print(topic_proportions.shape)   # D x K\n",
    "print(topics.shape)              # K x V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, one of the topics will mainly express the words \"broccoli\", \"eat\", and \"bananas\" with higher percentage, whereas the other topic will be mostly about \"cute\", \"hamsters\", and \"kittens\". This is consistent with our earlier intuitions of having a topic about animals and another topic about food.\n",
    "\n",
    "Recall that a topic is formally defined as a distribution over the entire vocabulary.\n",
    "\n",
    "##### Obtain the topic proportions\n",
    "\n",
    "We now want to find the topic proportions of each individual document. For instance, we know that document 1 is mostly about food, while document 4 is mostly about animals. The following commands allow us to obtain the topic distribution of each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build id2term (inverse dictionary)\n",
    "id2term = {v: k for k, v in # FILL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topics\n",
    "n_max = 5\n",
    "for kk in range(n_topics):\n",
    "    print('+ Topic ' + str(kk) + ':')\n",
    "    idx = np.argsort(-topics[kk,:])\n",
    "    print_str = ''\n",
    "    for nn in range(n_max):\n",
    "        print_str += id2term[idx[nn]] + ' '\n",
    "    print('   ' + print_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for d in range(D):\n",
    "    print(d,# FILL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1st and 2nd documents are mostly about food. The remaining two are instead about animals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Apply to new documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this can be applied to unseen documents too. For instance, consider the following new document, which is about both animals and food:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5 = \"Look at these hamsters munching on a piece of broccoli\".lower()\n",
    "doc5_tokenized=# FILL\n",
    "print(doc5_tokenized.shape)\n",
    "doc5_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.# FILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting topic proportions should be around $0.5$ (at least moderately close; keep in mind that these are all very short documents), indicating that this document expresses both topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show topics over document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "idx_D = np.arange(D)   # x-axis locations\n",
    "bar_width = 0.5\n",
    "plots = []\n",
    "height_cumulative = np.zeros(D)\n",
    "for kk in range(n_topics):\n",
    "    color = plt.cm.coolwarm(kk/n_topics, 1)\n",
    "    if kk==0:\n",
    "        p = plt.bar(idx_D, topic_proportions[:, kk], bar_width, color=color)\n",
    "    else:\n",
    "        p = plt.bar(idx_D, topic_proportions[:, kk], bar_width, bottom=height_cumulative, color=color)\n",
    "    height_cumulative += topic_proportions[:, kk]\n",
    "    plots.append(p)\n",
    "plt.ylim((0, 1))  # proportions sum to 1\n",
    "plt.ylabel('Topic proportions')\n",
    "plt.title('Topic proportions in documents')\n",
    "plt.yticks(np.arange(0, 1, 10))\n",
    "plt.xticks([0,1,2,3], labels=[1,2,3,4])\n",
    "plt.xlabel('Documents')\n",
    "topic_labels = ['Topic {}'.format(kk) for kk in range(n_topics)]\n",
    "plt.legend([p[0] for p in plots], topic_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.pcolor(topic_proportions, norm=None, cmap='Blues')\n",
    "topic_labels = ['Topic {}'.format(kk) for kk in range(n_topics)]\n",
    "plt.xticks(np.arange(topic_proportions.shape[1])+0.5, topic_labels)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(np.arange(topic_proportions.shape[0])+0.5, [1,2,3,4])\n",
    "plt.ylabel('Documents')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot topic proportions individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "for kk in range(n_topics):\n",
    "    plt.subplot(1, 2, kk+1)\n",
    "    plt.scatter(np.arange(D), topic_proportions[:, kk])\n",
    "    plt.ylim((0, 1))\n",
    "    plt.ylabel('Proportions')\n",
    "    plt.title('Topic '+str(kk))\n",
    "    if kk+2>=n_topics:\n",
    "        plt.xticks(np.arange(D), [d for d in range(D)] )\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(np.arange(0, 1, 10))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show words over topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = [x[0] for x in vocabulary_list]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(topics, cmap='Blues')\n",
    "plt.xticks(np.arange(V), labels=words, rotation=90)\n",
    "plt.yticks(np.arange(topic_proportions.shape[1]), ['Topic 0', 'Topic 1'])\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: analyze real dataset of NY Times articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Run a bash script from terminal:  \n",
    "`    tail -n +4 docword.nytimes.txt > nytimes.txt`  \n",
    "This will remove the first 3 lines from the file.  \n",
    "The format of `nytimes.txt` is 3 columns:  \n",
    "* 1st : document id\n",
    "* 2nd : word id\n",
    "* 3rd : frequency of word in that document  \n",
    "For instance the first lines are:  \n",
    "`1 413 1\n",
    "1 534 1\n",
    "1 2340 1\n",
    "1 2806 1\n",
    "1 3059 1\n",
    "1 3070 1\n",
    "1 3294 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Import data into the proper format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0=pd.read_csv('nytimes.txt',sep='\\s+', header=None,names=['docId','wordId','wordFreq'])\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the dataset size to speed up implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_D=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df0[df0.docId<=max_D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=max(df.docId.unique())\n",
    "V=max(df.wordId.unique())\n",
    "D,V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform into a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_nyt=# FILL\n",
    "corpus_nyt.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voc=pd.read_csv('vocab.nytimes.txt',header=None)\n",
    "df_voc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit LDA\n",
    "n_topics = 100\n",
    "lda_model_nyt = # FILL\n",
    "topic_proportions =# FILL\n",
    "topics =# FILL\n",
    "\n",
    "# Print log-likelihood\n",
    "print('\\nLog likelihood: ' + str(lda_model_nyt.score(# FILL)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topic proportions\n",
    "Documents with mostly one topic only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold=0.98\n",
    "docs_one_topic=[d for d in range(D) if max(topic_proportions[d])>threshold]\n",
    "main_topics=[# FILL]\n",
    "main_topics_histo=Counter(# FILL)\n",
    "\n",
    "df_topics_histo = pd.DataFrame.from_dict(main_topics_histo, orient='index')\n",
    "df_topics_histo=df_topics_histo.sort_values(by=[0],ascending=False) \n",
    "df_topics_histo.plot(kind='bar',figsize=(12,6))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize main topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_topics=list(df_topics_histo.iloc[:5].index)\n",
    "n_max = 10\n",
    "for kk in max_topics:\n",
    "    print('+ Topic ' + str(kk) + ':')\n",
    "    idx = np.argsort(-topics[kk,:])\n",
    "    print_str = ''\n",
    "    main_words_in_this_topic=list(np.concatenate([df_voc.iloc[idx[nn]].values for nn in range(n_max)]))\n",
    "#     a=list(np.concatenate(a))\n",
    "    print(main_words_in_this_topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize one doc that has only one topic.  \n",
    "Pick one that has a topic among the most frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=max_topics[0]\n",
    "possible_d=[d for d in docs_one_topic if # FILL]\n",
    "sample_d=np.random.choice(# FILL)\n",
    "print('Chosen doc:',sample_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_d += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_d=# FILL\n",
    "main_wordsId_in_this_doc=df_sample_d.iloc[:20]['wordId'].values\n",
    "main_words_in_this_doc=np.concatenate([df_voc.iloc[w].values for w in main_wordsId_in_this_doc])\n",
    "main_words_in_this_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show more topic proportions\n",
    "Pick a sample of documents and show their main topic proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "idx_D = np.arange(D)   # x-axis locations\n",
    "idx_D=np.arange(100) # max 100 documents\n",
    "bar_width = 0.5\n",
    "plots = []\n",
    "height_cumulative = np.zeros(max(idx_D)+1)\n",
    "\n",
    "idx = np.argsort(-topic_proportions[:max(idx_D)+1])[:,:5] # order the first 5 topics for each document\n",
    "for kk in range(5): # max 10 topics to visualize\n",
    "    color = plt.cm.coolwarm(kk/5, 1)\n",
    "    if kk==0:\n",
    "        p = plt.bar(idx_D, topic_proportions[idx_D, idx[:,kk]], bar_width, color=color)\n",
    "    else:\n",
    "        p = plt.bar(idx_D, topic_proportions[idx_D, idx[:,kk]], bar_width, bottom=height_cumulative, color=color)\n",
    "    height_cumulative += topic_proportions[idx_D, idx[idx_D,kk]]\n",
    "    plots.append(p)\n",
    "plt.ylim((0, 1))  # proportions sum to 1\n",
    "plt.ylabel('Topic proportions')\n",
    "plt.title('Topic proportions in documents')\n",
    "plt.yticks(np.arange(0, 1, 10))\n",
    "topic_labels = ['Topic {}'.format(kk) for kk in range(n_topics)]\n",
    "plt.legend([p[0] for p in plots], topic_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualize HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "plt.pcolor(topic_proportions, norm=None, cmap='Blues')\n",
    "topic_labels = ['Topic {}'.format(kk) for kk in range(n_topics)]\n",
    "plt.xticks(np.arange(topic_proportions.shape[1])+0.5, topic_labels);\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xticks(rotation=90)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weighted impact of topic over documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_impact=topic_proportions.sum(axis=0)\n",
    "most_impactful_topics=np.argsort(-topic_proportions.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize least impactful topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max=10\n",
    "for t in most_impactful_topics[-5:]:\n",
    "    idx = np.argsort(-topics[t,:])\n",
    "    print_str = ''\n",
    "    main_words_in_this_topic=list(np.concatenate([df_voc.iloc[idx[nn]].values for nn in range(n_max)]))\n",
    "#     a=list(np.concatenate(a))\n",
    "    print(main_words_in_this_topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize documents containing the least impactful topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_least=most_impactful_topics[-1]\n",
    "d_least=np.argsort(-topic_proportions[:,t_least])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_proportions[d_least,t_least]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_least : topic least impactful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = # FILL\n",
    "main_words_in_this_topic=# FILL\n",
    "print(main_words_in_this_topic)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d_least : document including topic least impactful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_d=# FILL\n",
    "main_wordsId_in_this_doc=df_sample_d.iloc[:100]['wordId'].values\n",
    "main_words_in_this_doc=np.concatenate([df_voc.iloc[w].values for w in main_wordsId_in_this_doc])\n",
    "main_words_in_this_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize most impactful topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_topics=most_impactful_topics[:5]\n",
    "n_max = 10\n",
    "for kk in max_topics:\n",
    "    print('+ Topic ' + str(kk) + ':')\n",
    "    idx = np.argsort(-topics[kk,:])\n",
    "    print_str = ''\n",
    "    main_words_in_this_topic=list(np.concatenate([df_voc.iloc[idx[nn]].values for nn in range(n_max)]))\n",
    "#     a=list(np.concatenate(a))\n",
    "    print(main_words_in_this_topic)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
