# Advanced Probabilistic Machine Learning and Applications (2022)

  Winter semester 2021/2022.
  
## Course information

**Lecturer**:  [Caterina De Bacco](https://www.cdebacco.com/)

For any general question about the course, use GitHub issues. Before posting, please make sure your question has not been previously answered. Only in case of private question, send us an email. 

**Plan**:  Thu 21st Oct  2021 - Fry 11th Feb 2022, 14 weeks, 2+2 hr/week, 14 weeks, 56hr

**Lectures**:  Thursdays 14:15-16pm in presence (if possible), alternatively online (in case the url to the class will be added here). 

**Tutorials**: Fridays 14:16-16.00pm in presence (if possible), alternatively online (in case the url to the class will be added here). 

**Feedback**: after every lecture you are free to give a feedback.

**Lecture-free days**: Friday, from Dec 24th 2021 bis Sat Jan 8th 2022 (Weihnachtspause)  

**Prerequisites**: all the previous knowledge required by the class Probabilistic Machine Learning applies here too. Nothing extra is needed, so if you are following that class, you are good to go here too.
There will be some overlapping contents for sure between the two classes, although my class is more focused on the application side.

For the EXAM, NEED to officially register  either via Campus / ALMA or written if the student cannot register online (closer to the exam date).  

**Grading** : Maximum between 70\% written exam+30\% assignments and 100\% exam.   

* Every assignment is composed by several exercises, which will be released sequentially. Information about assignment submission will be provided later in time but it will be made electronically. 

* Assignment may be done and submitted in groups of up to 3 people (optional). 

### Tentative program and schedule

 1.  **Introduction to probabilistic machine learning** 
     * _Reference_: Chapter 2 up to Section 2.3.6 and Section 8.2 of Bishop	
2.  **Gaussian Mixture Model (GMM) + Expectation Maximization** 
    * _Reference_: Section 9.2 of Bishop 
3.  **Bayesian Mixture Models + Gibbs Sampling** 
4.  **Mean Field approach** 
    * _Reference_: AMFM
5.  **TAP approximation** 
    * _Reference_: AMFM
6.  **Bethe Approximation and Belief Propagation part I** 
    * _Reference_: MM  
7. **Bethe Approximation and Belief Propagation part II** 
8. **Stochastic Block Model**  
9. **GMMs + Variational Inference (VI)** 
10. **Poisson matrix factorization**
11. **Probabilistic matrix factorization** for recommender systems
12. **VI + LDA** 
13. **Advanced VI: Stochastic VI and Black Box VI** 
    

### References

* Bishop=C. M. Bishop, _Pattern recognition and machine learning_ (Springer, 2006).
* AMFM=M. Opper and D. Saad, _Advanced mean field methods: Theory and practice_ (MIT press, 2001).
* MM= M. MÃ¨zard and A. Montanari, _Information, Physics and Computation_ (Oxford Graduate texts, 2009).
