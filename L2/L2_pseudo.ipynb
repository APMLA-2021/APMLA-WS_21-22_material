{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: Mixture Models and Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Categorical Mixture Model (CMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from categorical_em import CategoricalEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4) Play around with the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load and pre-process the data\n",
    "Load the data from the tweets_cleaned csv file as `pandas` dataframe. It contains the documents already pre-processed and cleaned after applying the following steps:\n",
    "\n",
    "1. Tokenization\n",
    "2. Homogeneization, which includes:\n",
    "    1. Removing capitalization.\n",
    "    2. Removing non alphanumeric tokens (e.g. punktuation signs)\n",
    "    3. Stemming/Lemmatisation.\n",
    "3. Cleaning\n",
    "4. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = # FILL HERE\n",
    "# FILL HERE  # drop duplicates tweets\n",
    "df['tokens'] = df['tokens'].apply(# FILL HERE)  # transform the string into a list of tokens\n",
    "X_tokens = list(df['tokens'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print('Columns: {}\\n'.format(' | '.join(df.columns.values)))\n",
    "\n",
    "print('Tweet:\\n{}'.format(df.loc[1, 'tweet']))\n",
    "print('Tweet cleaned:\\n{}'.format(df.loc[1, 'tweets_clean']))\n",
    "print('Tweet tokens:\\n{}'.format(X_tokens[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the dictionary\n",
    "We have transformed the raw text collection in a list of documents stored in `X_tokens`, where each document is a collection of words which are the most relevant according to the semantic analysis. \n",
    "\n",
    "We now convert these data (a list of token lists) into a numerical representation (a list of vectors, or a matrix). For this purpose we use the `gensim` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 120  # hyperparameter: number of different words to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.# FILL HERE\n",
    "print(dictionary)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=I)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Bag of Words (BoW)\n",
    "Let's create the numerical version of our corpus using the `doc2bow` method. In general, \n",
    "`D.doc2bow(token_list)` transforms any list of tokens into a list of tuples `(token_id, n)`, one per each token in \n",
    "`token_list`, where `token_id` is the token identifier (according to dictionary `D`) and `n` is the number of occurrences \n",
    "of such token in `token_list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_bow = list()\n",
    "keep_tweet = list()\n",
    "for tweet in X_tokens:\n",
    "    tweet_bow = # FILL HERE\n",
    "    if len(tweet_bow) > 1:\n",
    "        X_bow.append(tweet_bow)\n",
    "        keep_tweet.append(True)\n",
    "    else:\n",
    "        keep_tweet.append(False)\n",
    "\n",
    "df_data = df[keep_tweet]\n",
    "N = len(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create the matrix\n",
    "Finally, we transform the BoW representation `X_bow` into a matrix, namely `X_matrix`, in which the n-th row and j-th column represents the \n",
    "number of occurrences of the j-th word of the dictionary in the n-th document. This will be the matrix used in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_matrix = np.zeros([N, I])\n",
    "for i, doc_bow in enumerate(X_bow):\n",
    "    # FILL HERE\n",
    "X_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5) Implement the EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6  # hyperparameter: number of topics\n",
    "i_theta = 5\n",
    "i_pi = 5\n",
    "model = CategoricalEM(K, I, N, delta=0.01, epochs=200, init_params={'theta': i_theta, 'pi': i_pi})\n",
    "model.fit(X_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6) Show the ten most representative words for each topic using a wordcloud, and the ten most relevant documents for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FILL HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(30, 10))\n",
    "for k in range(K):\n",
    "    # FILL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnk = # FILL HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILL HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
